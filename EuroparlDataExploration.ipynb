{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir dataset\n",
    "# cd dataset\n",
    "# wget --no-check-certificate https://www.statmt.org/europarl/v7/es-en.tgz\n",
    "# tar -zxvf es-en.tgz\n",
    "# These files names do not exactly match the ones below, so maybe wrong file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# chose a random value to get the code to work\n",
    "MAX_VOCABULARY = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EuroParl English-Spanish Dataset\n",
    "\n",
    "In this notebook, I explore the EuroParl dataset for English/Spanish translation. My goal is to gain a somewhat good understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "First, we load the datasets. There are two files provided when we download the Europarl dataset: `Europarl.<lan_1>-<lan-2>.<lan_1>` and `Europarl.<lan_1>-<lan-2>.<lan_2>` It is up to the user to decide which one is the data and which one are the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length in Words: EN: 294,629,416 | ES: 318,950,453 | Difference: 24,321,037\n",
      "Number of Sentences: EN: 1,965,735 | ES: 1,965,735\n",
      "Sample Translation:\n",
      "\n",
      "\tanother positive aspect is the possibility for each member state to define protection areas and risk areas.\n",
      "\n",
      "\totro aspecto positivo es la posibilidad de que cada estado miembro defina sus áreas de protección y áreas de riesgo.\n"
     ]
    }
   ],
   "source": [
    "# europarl_english_path = \"dataset/Europarl.en-es.en\"\n",
    "europarl_english_path = \"dataset/europarl-v7.es-en.en\"\n",
    "\n",
    "# Load datasets from file\n",
    "with open(europarl_english_path, 'r') as corpus:\n",
    "    english_corpus = corpus.read().lower()\n",
    "    \n",
    "# europarl_spanish_path = \"dataset/Europarl.en-es.es\"\n",
    "europarl_spanish_path = \"dataset/europarl-v7.es-en.es\"\n",
    "with open(europarl_spanish_path, 'r') as corpus:\n",
    "    spanish_corpus = corpus.read().lower()\n",
    "\n",
    "# Split corpi into sentences for later training and inference.\n",
    "english = english_corpus.split('\\n')\n",
    "spanish = spanish_corpus.split('\\n')\n",
    "\n",
    "assert len(english) == len(spanish), f\"Number of sentences between both languages is not equal! {len(english):,} vs {len(spanish):,}\"\n",
    "total_sentences = len(english)\n",
    "    \n",
    "# Some basic statistics about the corpi\n",
    "print(f'Length in Words: EN: {len(english_corpus):,} | ES: {len(spanish_corpus):,} | Difference: {abs(len(english_corpus) - len(spanish_corpus)):,}')\n",
    "print(f'Number of Sentences: EN: {len(english):,} | ES: {len(spanish):,}')\n",
    "\n",
    "# Show an example sentence trainslation\n",
    "import random\n",
    "\n",
    "ran_sen = random.randint(0, total_sentences)\n",
    "sample_english = english[ran_sen]\n",
    "sample_spanish = spanish[ran_sen]\n",
    "\n",
    "print(f'Sample Translation:\\n')\n",
    "print(f'\\t{sample_english}\\n')\n",
    "print(f'\\t{sample_spanish}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Sentences and Collect Vocabulary\n",
    "\n",
    "Tokenization is helpful for spliting the words and the special characters to feed into the model so that it can process it. In out case, we split between spaces and special characters. For example, the sentence `Today's dollar.` would be split into -> [`today`, `'`, `s`, `dollar`, `.`].\n",
    "\n",
    "Further, the vocabulary is obtained for each language. This vocabulary will contain the `MAX_VOCABULARY` most populat words in each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on English...\n",
      "Working on Spanish...\n"
     ]
    }
   ],
   "source": [
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "\n",
    "print('Working on English...')\n",
    "english_sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in english ]\n",
    "# Removes tokens that were empty. This helps in separating special characters!\n",
    "english_tokens = [[w for w in s if len(w)] for s in english_sentences]\n",
    "\n",
    "# English vocabulary calculation\n",
    "english_words = [w for s in english_tokens for w in s]\n",
    "\n",
    "english_vocab = Counter(english_words).most_common(MAX_VOCABULARY)\n",
    "english_vocab = [w[0] for w in english_vocab]\n",
    "\n",
    "print('Working on Spanish...')\n",
    "spanish_sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in spanish ]\n",
    "# Removes tokens that were empty. This helps in separating special characters!\n",
    "spanish_tokens = [[w for w in s if len(w)] for s in spanish_sentences]\n",
    "\n",
    "# Spanish vocabulary calculation\n",
    "spanish_words = [w for s in spanish_tokens for w in s]\n",
    "\n",
    "spanish_vocab = Counter(spanish_words).most_common(MAX_VOCABULARY)\n",
    "spanish_vocab = [w[0] for w in spanish_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Languages\n",
    "\n",
    "This is helpful for loading the languages later, skipping over all of the above processing from the raw files.\n",
    "\n",
    "The sentences are stored in a dictionary, separating among actual tokenized instances, vocabulary, and a flag over the language. Numerical information such as the number of sentences or MAX_VOCABULARY can be extracted from the data itself, so it is not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_languages = True\n",
    "\n",
    "english_europarl = {\n",
    "    'tokens':     english_tokens,\n",
    "    'vocabulary': english_vocab,\n",
    "    'language': 'en',\n",
    "}\n",
    "\n",
    "spanish_europarl = {\n",
    "    'tokens':     spanish_tokens,\n",
    "    'vocabulary': spanish_vocab,\n",
    "    'language': 'es',\n",
    "}\n",
    "\n",
    "if save_languages:\n",
    "    \n",
    "    with open('dataset/Europarl.en.pkl', 'wb') as jar:\n",
    "        pickle.dump(english_europarl, jar)\n",
    "        \n",
    "    with open('dataset/Europarl.es.pkl', 'wb') as jar:\n",
    "        pickle.dump(spanish_europarl, jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'that', 'a', 'is']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_europarl['vocabulary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
